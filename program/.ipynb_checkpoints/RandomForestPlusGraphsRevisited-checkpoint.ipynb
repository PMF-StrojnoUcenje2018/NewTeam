{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd22fd1c970a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    global X_train, y_train, X_test, y_test, train_ind, test_ind, model_full \n",
    "    global feature_columns, response_column, n_features\n",
    "    \n",
    "    model_full = pd.read_csv('df.csv')\n",
    "    \n",
    "    key = 'is packer'\n",
    "    keyOut = 'packer type'\n",
    "\n",
    "    response_column = key\n",
    "    feature_columns = list(model_full)\n",
    "    feature_columns.remove('name')\n",
    "    feature_columns.remove(key)\n",
    "    feature_columns.remove(keyOut)\n",
    "    \n",
    "    #naknadno izbaƒçeni featuresi\n",
    "    feature_columns.remove('log_max_heap_reserve')\n",
    "    feature_columns.remove('log_repeated_entry_sizes')\n",
    "    feature_columns.remove('non_empty_metadatas')\n",
    "    feature_columns.remove('certificate')\n",
    "    feature_columns.remove('cert_malformed')\n",
    "    feature_columns.remove('rich_header_valid')\n",
    "    feature_columns.remove('max_opt_header_size')\n",
    "   \n",
    "    n_features = len(feature_columns)\n",
    "\n",
    "    names = np.arange(model_full.shape[0])\n",
    "    train_full, test_full, train_ind, test_ind = train_test_split(model_full, names, test_size=0.34, random_state=0)\n",
    "\n",
    "    X_train  = train_full[feature_columns]\n",
    "    y_train  = train_full[response_column]\n",
    "    X_test   = test_full[feature_columns]\n",
    "    y_test   = test_full[response_column]\n",
    "            \n",
    "    print('Number of train data: ', X_train.shape[0])\n",
    "    print('Number of test data:  ', X_test.shape[0])\n",
    "    print('Number of features:  ', n_features)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_Curve(rf, auc):\n",
    "    y_predicted = rf.predict_proba(X_test)[:, 1]\n",
    "    false_positive, true_positive, _ = metrics.roc_curve(y_test, y_predicted)\n",
    "\n",
    "    f = plt.figure()\n",
    "    f.set_size_inches(18.5, 10.5, forward=True)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(false_positive, true_positive, color='darkorange', label='Random Forest')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve (area = %0.6f)' % auc)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    #f.savefig(\"ROC_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_Metrics(model):\n",
    "    print('\\nModel performance on the test data set:')\n",
    "\n",
    "    y_predict_test  = np.asarray(model.predict(X_test))\n",
    "    mse             = metrics.mean_squared_error(y_test.astype(int), y_predict_test.astype(int))\n",
    "    logloss_test    = metrics.log_loss(y_test, y_predict_test)\n",
    "    accuracy_test   = metrics.accuracy_score(y_test, y_predict_test)\n",
    "    accuracy_test2  = model.score(X_test, y_test)\n",
    "    F1_test         = metrics.f1_score(y_test, y_predict_test)\n",
    "    precision_test  = metrics.precision_score(y_test, y_predict_test, average='binary')\n",
    "    precision_test2 = metrics.precision_score(y_test, y_predict_test)\n",
    "    recall_test     = metrics.recall_score(y_test, y_predict_test, average='binary')\n",
    "    auc_test        = metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    r2_test         = metrics.r2_score(y_test.astype(int), y_predict_test.astype(int))\n",
    "    \n",
    "    header = [\"Metric\", \"Test\"]\n",
    "    table  = [\n",
    "               [\"logloss\",   logloss_test],\n",
    "               [\"accuracy\",  accuracy_test],\n",
    "               [\"precision\", precision_test],\n",
    "               [\"F1\",        F1_test],\n",
    "               [\"r2\",        r2_test],\n",
    "               [\"AUC\",       auc_test]\n",
    "             ]\n",
    "\n",
    "    print(tabulate(table, header, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_predictor_importance(model, feature_columns):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    y_pos  = np.arange(feature_importance.shape[0]) + .5\n",
    "    fig, ax = plt.subplots()\n",
    "    f = fig\n",
    "    fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "    ax.barh(y_pos, \n",
    "            feature_importance, \n",
    "            align='center', \n",
    "            color='green', \n",
    "            ecolor='black', \n",
    "            height=0.5)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feature_columns)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Relative Importance')\n",
    "    ax.set_title('Predictor Importance')\n",
    "    plt.show()\n",
    "    #f.savefig(\"feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_confusion_matrix(cm, auc, heading):\n",
    "    print('\\n', heading)\n",
    "    print(cm)\n",
    "    true_negative  = cm[0,0]\n",
    "    true_positive  = cm[1,1]\n",
    "    false_negative = cm[1,0]\n",
    "    false_positive = cm[0,1]\n",
    "    total = true_negative + true_positive + false_negative + false_positive\n",
    "    accuracy = (true_positive + true_negative)/total\n",
    "    precision = (true_positive)/(true_positive + false_positive)\n",
    "    recall = (true_positive)/(true_positive + false_negative)\n",
    "    misclassification_rate = (false_positive + false_negative)/total\n",
    "    F1 = (2*true_positive)/(2*true_positive + false_positive + false_negative)\n",
    "    print('accuracy.................%7.4f' % accuracy)\n",
    "    print('precision................%7.4f' % precision)\n",
    "    print('recall...................%7.4f' % recall)\n",
    "    print('F1.......................%7.4f' % F1)\n",
    "    print('auc......................%7.4f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_learning_curve(model, title, X, y, ylim = None, cv = None,\n",
    "                        n_jobs = 1, train_sizes = np.linspace(0.1, 1.0, 5)):\n",
    "    f = plt.figure()\n",
    "    f.set_size_inches(18.5, 10.5, forward=True)\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, \n",
    "                                                            X, y,\n",
    "                                                            cv = cv,\n",
    "                                                            n_jobs = n_jobs,\n",
    "                                                            train_sizes = train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    #f.savefig(\"learning_curve.png\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test():\n",
    "    global model\n",
    "    global y_predicted_test\n",
    "\n",
    "    # Instantiate model\n",
    "    rfc = RandomForestClassifier(n_estimators=800, \n",
    "                             random_state=42, \n",
    "                             criterion = 'entropy',#default='gini' \n",
    "                             min_samples_split =4, #default=2\n",
    "                             bootstrap = False,    #uzorkovanje bez ponavljanja\n",
    "                              \n",
    "                             n_jobs=-1, #rad na maksimalnom broju procesora\n",
    "                            )\n",
    "\n",
    "    fit = rfc.fit(X_train, y_train)\n",
    "    print('\\nTraining finished\\n')\n",
    "    \n",
    "    # Cross validation with 20 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    #cv_ = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n",
    "    #Plot_learning_curve(rfc, \n",
    "    #                    'Learning Curve',\n",
    "    #                    model_full[feature_columns], model_full[response_column], \n",
    "    #                    cv = cv_,\n",
    "    #                    n_jobs = 1)\n",
    "    \n",
    "    model = rfc\n",
    "    print('\\nModel:\\n', model)\n",
    "\n",
    "    print('\\nFeature Importances:', model.feature_importances_)\n",
    "    Plot_predictor_importance(model, feature_columns)\n",
    "\n",
    "    y_predicted = model.predict(X_train)\n",
    "    probabilities = model.predict_proba(X_train)\n",
    "\n",
    "    c_report = metrics.classification_report(y_train, y_predicted)\n",
    "    print('\\nClassification report:\\n', c_report)\n",
    "\n",
    "    y_predicted_train = model.predict(X_train)\n",
    "    cm = metrics.confusion_matrix(y_train, y_predicted_train)\n",
    "    auc = metrics.roc_auc_score(y_train, y_predicted_train)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the training dataset')\n",
    "\n",
    "    y_predicted = model.predict(X_test)\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    auc = metrics.roc_auc_score(y_test, y_predicted)\n",
    "\n",
    "    ntotal = len(y_test)\n",
    "    correct = y_test == y_predicted\n",
    "    numCorrect = sum(correct)\n",
    "    percent = round( (100.0*numCorrect)/ntotal, 6)\n",
    "    print(\"\\nCorrect classifications on test data: {0:d}/{1:d} {2:8.3f}%\".format(numCorrect, ntotal, percent))\n",
    "    prediction_score = 100.0*model.score(X_test, y_test)\n",
    "    print('Random Forest Prediction Score on test data: %8.3f' % prediction_score)\n",
    "    \n",
    "    y_predicted_test = model.predict(X_test)\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted_test)\n",
    "    auc = metrics.roc_auc_score(y_test, y_predicted_test)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the test dataset')\n",
    "    ROC_Curve(model, auc)\n",
    "    Print_Metrics(model)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LoadData()\n",
    "Train_Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podaciProba = {'prediction' : y_predicted_test,\n",
    "              'original' : y_test}\n",
    "podaciProba = pd.DataFrame(podaciProba)\n",
    "tablica = pd.crosstab(podaciProba['original'], podaciProba['prediction'])\n",
    "\n",
    "#tablica.plot.bar()\n",
    "print('Postotak pogoƒëenih:')\n",
    "print(sum(y_predicted_test==y_test)/len(y_test))\n",
    "tablica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podaciProba = {'rezultati' : y_predicted_test,\n",
    "              'originali' : model_full['packer type'][test_ind]}\n",
    "podaciProba = pd.DataFrame(podaciProba)\n",
    "tablica = pd.crosstab(podaciProba['originali'], podaciProba['rezultati'])\n",
    "\n",
    "#tablica.plot.bar()\n",
    "display(tablica)\n",
    "\n",
    "#postoci po klasama\n",
    "column1 = tablica[0]/tablica.apply(sum, axis=1) * 100\n",
    "column2 = tablica[1]/tablica.apply(sum, axis=1) * 100\n",
    "\n",
    "column3 = column1 + column2\n",
    "\n",
    "d = {'False': column1, 'True': column2, 'Ukupno': column3}\n",
    "df = pd.DataFrame(data=d)\n",
    "print('\\n\\n' + '\\033[1m' + 'Postotci po tipu packera:' + '\\033[0m')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_not_packed = column1[0]/100\n",
    "#print(r_not_packed)\n",
    "r_overlay_and_cripters = (tablica[1][1] + tablica[1][2])/(tablica[0][1] + tablica[0][2]+tablica[1][1] + tablica[1][2])\n",
    "#print(r_overlay_and_cripters)\n",
    "r_protectors = column2[3]/100\n",
    "#print(r_protectors)\n",
    "r_compress = column2[4]/100\n",
    "#print(r_compress)\n",
    "\n",
    "bodovi = 8 * r_not_packed**3 * r_overlay_and_cripters * r_protectors * r_compress\n",
    "print('Bodovi za kvalitetu rjesenja '+str(bodovi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
